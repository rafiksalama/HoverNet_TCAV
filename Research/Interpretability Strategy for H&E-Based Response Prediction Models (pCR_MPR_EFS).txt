Interpretability Strategy for H&E-Based Response Prediction Models (pCR/MPR/EFS) Overview and Goals The challenge is to explain predictions from a whole-slide H&E model that outperforms clinical-only models in predicting pathological complete response (pCR), major pathological response (MPR), and event-free survival (EFS). We have a pipeline where gigapixel WSIs are embedded by pathology foundation models (e.g. CONCH, THREADS, TITAN) and then fed into a multiple instance learning (MIL) classifier. While performance is high, we need to ensure the model is “right for the right reasons.” This means linking the model’s visual focus to human-understandable histologic features, avoiding spurious correlations (artifacts, batch effects), and providing explanations that can be audited in clinical deployment. Key objectives: (1) Identify interpretation methods suited to WSI-level models, beyond basic heatmaps. (2) Validate these explanations for faithfulness (causally reflect the model’s logic), stability (robust across sites/stains), and clinical plausibility (align with pathology knowledge). (3) Outline a practical playbook for integrating these methods, including data needed, expected outputs, success criteria, and a rollout plan. Below we survey state-of-the-art WSI interpretability techniques and propose how to combine them into a comprehensive interpretability solution. Interpretability Methods Landscape (WSI & MIL Models) MIL Attention Heatmaps (Localization of Model Focus) Weakly-supervised MIL models (like attention-based MIL) inherently provide attention scores for image patches, indicating “where the model is looking.” For instance, the CLAM model (Clustering-constrained Attention MIL) learns to assign higher weights to subregions of high diagnostic value . These attention heatmaps can highlight tumor regions or other features that drive slide-level predictions. In the CLAM paper, attention maps were shown to consistently correspond to tumor tissue and even outline tumor boundaries without any pixel-level labels . Such heatmaps give a first-level explanation by pinpointing important areas on the WSI. To use attention explanations effectively, we should: • Visualize high-attention regions: Generate heatmaps over the WSI (e.g. by mapping normalized attention scores to spatial locations ). A pathologist can review if the highlighted regions make sense (e.g., tumor beds with many lymphocytes for a pCR prediction). • Validate via ablation: As a faithfulness test, mask out top-attended tiles and see if the model confidence drops. A large drop in performance when important tiles are removed would confirm the heatmap’s causal relevance. • Augment with spatial context: Standard MIL ignores spatial relationships, but recent variants incorporate spatial coherence to avoid focusing on isolated single tiles . We might consider enforcing that high-attention tiles form contiguous regions (since morphologic patterns like necrosis or lymphocytic infiltrates span areas). Pros: Attention heatmaps are readily available from our model and give an intuitive “importance map” . They have been used to identify known morphological features in WSIs . Cons: Raw heatmaps alone can be misleading – they may highlight correlated artifacts or be diffused and hard to interpret biologically. Also, attention is a soft indicator; it doesn’t name what pattern is important, just where. Therefore, we treat heatmaps as a starting point, to be enhanced with concept recognition and rigorous checks (see validation section below). Prototype and Case-Based Explanations (“This looks like that”) Prototype-based models offer case-based reasoning, presenting human-interpretable prototypes that the model uses for decisions. A ProtoPNet, for example, learns a set of prototypical image patches during training; each prototype is a representative patch that the model finds important for a class . At inference, the model essentially says “this region in the slide looks like that prototype”, where “that prototype” is a patch with a known class or outcome association. This yields an explanation in terms of similar past cases or patterns . Prototype MIL approaches (e.g. ProtoMIL or PAMIL) extend this idea to WSI classification by learning prototypes at the patch level and weighting instance contributions based on prototype similarity How to use prototypes in our context: • Integrate a prototype layer: We can attach a prototype module to the embedding features. For instance, learn $m$ prototypes for “pCR-associated patterns” and “non-pCR patterns.” During prediction, the model can show the top $k$ slide patches that activated each prototype, along with the actual prototype image (which comes from a training slide). • Prototype visualization: Displaying the prototype images side by side with the query slide’s patches can be powerful. For example, the model might highlight a region in the new slide and say it matches a prototype that (on a training slide) was a lymphocyte-rich tumor stroma patch that strongly indicated pCR. This provides a specific, human-reviewable piece of evidence for the model’s reasoning . • Prevent trivial prototypes: Ensure prototypes aren’t capturing dataset-specific artifacts. (One known issue is prototypes could latch onto easy-to-learn differences like slide color if not careful.) We can add diversity constraints or manually inspect prototypes to confirm they correspond to meaningful histology (tumor cells, necrosis, etc., rather than a slide barcode or pen mark). Pros: Prototype explanations are inherently interpretable because the model’s prediction is literally based on comparisons to visible examples . This is akin to how pathologists reason (e.g. “this tumor’s morphology resembles case X”). It provides both localization (the region in question on the slide) and a reference image for context. Cons: Without concept labeling, a prototype is just an image – it still requires a person to interpret why that prototype is relevant. Also, if the number of prototypes is large, it can become unwieldy; we should aim for a concise set of salient prototypes. Moreover, purely example-based explanation doesn’t articulate which feature of the image is important – it just says two patches look alike. Combining prototypes with concept annotations could address this (e.g. tag a prototype as “sheet of necrosis” vs “lymphocytic aggregate around tumor”). Concept-Based Explanations (Human-Readable Features) Rather than only giving low-level visualizations, concept-based methods seek to explain predictions in terms of high-level semantic concepts – e.g. “tumor differentiation”, “necrosis extent”, “immune infiltration”. This directly addresses the need to link model signals to pathologist-understandable morphology. Testing with Concept Activation Vectors (TCAV): TCAV is a popular post-hoc technique that asks: How sensitive is the model’s prediction to a user-defined concept? . For example, we can define a concept “Tumor-Infiltrating Lymphocytes (TILs)” by assembling a set of image patches rich in lymphocytes (and a set of patches without TILs as negatives). TCAV uses the model’s internal embeddings (e.g. the foundation model features) to learn a vector for this concept, and then calculates a TCAV score which is essentially the fraction of model predictions that increase when the input is “perturbed” toward that concept . A high TCAV score for concept “TIL density” in predicting pCR would mean the model’s positive pCR predictions are strongly associated with TIL-rich regions – a quantitative concept attribution. • Implementation: Using our existing feature extractor (e.g. CONCH embeddings of patches), gather concept datasets for each concept of interest (could use expert-labeled regions or even external data). Compute concept activation vectors and concept importances for each outcome. This will tell us, for instance, that “necrosis presence” had a TCAV score of 0.85 for pCR prediction, implying the model significantly relies on necrosis to call a complete response. • Multiple concepts: We should test a broad panel of morphologic concepts: Tumor viability (residual tumor vs fibrosis), Inflammation (lymphocyte or plasma cell infiltrates), Tumor-Stroma ratio, Mitotic activity, Differentiation grade, etc. Each concept’s influence on each outcome can be computed. This yields a concept profile for the model. Automatic Concept Discovery (ACE): One challenge is we might not know all the salient concepts a priori. ACE (Automatic Concept-Based Explanations) addresses this by discovering concepts directly from the model’s internal features . It clusters the activations to find distinct groups (potentially corresponding to some morphology), then retrieves image patches representing each cluster. A pathologist can then assign names to these clusters – e.g. cluster 1 might turn out to be “tumor gland formation”, cluster 2 “necrotic debris”, cluster 3 “lymphoid aggregate”, etc. Once named and validated, these discovered concepts can be fed back into TCAV (using the clustered patches as concept examples) to quantify their importance. This way, we aren’t limited to preconceived notions of features; we let the model show us what it “sees” and then interpret it in pathology terms. Concept Bottleneck Models (CBM): Instead of post-hoc analysis, CBMs incorporate concepts into the model itself. In a CBM, the model is trained in two stages – first to predict a set of human-defined concept labels for each case, and then to predict the outcome from those concepts . For instance, we might train it to predict slide-level scores like “% area necrosis, % lymphocyte, tumor differentiation= poor/moderate/etc, presence of tertiary lymphoid structures (yes/no)” and so on, and then use those predictions to predict pCR. The advantage is that the internal representation is entirely interpretable – at test time, the model produces concept values which we can examine (and even manually adjust to do counterfactual reasoning) . If the model is uncertain about a concept, a human could step in and correct it, then see updated predictions . • In practice, training a full CBM might require concept annotations for a decent number of slides, which we may or may not have. A compromise is a semi-supervised CBM: use strong features (foundation model) and a small set of concept-labeled slides to train concept predictors, then propagate those concept predictions across the dataset. • Even if CBM performance doesn’t match the black-box model, it can serve as an approximation tool: e.g., we could check if the black-box model’s predictions correlate well with certain concept bottleneck model outputs (signifying alignment with those concepts). Pros: Concept-based explanations directly address the “what is the model seeing” question in high-level terms. TCAV has been shown provide meaningful explanations in medical imaging by highlighting clinically relevant features . Concept bottlenecks enable counterfactual analysis (e.g., “if this slide had no necrosis, would the prediction change?”) and impose an interpretable structure by design Cons: They require defining or discovering the right concepts and obtaining concept labels or examples. If concepts are too coarse or mislabeled, the explanations might be misleading. Also, TCAV and ACE are post-hoc and partial – they explain the model in terms of chosen concepts, but there could be other unknown factors the model uses. We mitigate this by combining concept analysis with artifact auditing (below) to catch non-biological factors. Data Attribution: Tracing Predictions to Training Data Another powerful explanation approach is to examine which training examples most influenced the model’s prediction on a given slide. If we can identify that, we might discover whether the model is picking up on spurious correlations present in certain training slides. Influence Functions: As proposed by Koh and Liang, influence functions approximate the effect of removing a particular training sample on the model’s predictions . By computing the influence score for each training WSI with respect to a test WSI, we can rank training slides by how much they positively or negatively influenced the test prediction. In practical terms, if a test slide is predicted to be pCR, and we find the top influential training slides are all pCR cases with very mucinous histology, it hints the model may be using mucin as a cue. Similarly, we might find a single lab’s slides are heavily influencing – indicating a batch effect. TracIn: This is an efficient approximation of influence that accumulates influence over training iterations. It’s easier to implement for deep networks (which don’t strictly satisfy the assumptions of classic influence functions but can still yield useful signals). TracIn would let us trace, for each test slide, which training patches and slides were most similar in feature-space and had high gradient influence. Using data attribution in our plan: • For a selection of cases (especially borderline or high-confidence cases), compute the top-N supportive and top-N detracting training examples. Provide these to the pathologist: “The model’s decision for this slide was most influenced by training slide #A (which had artifact X) and slide #B (which had outcome Y).” • This can surface shortcuts. E.g., if we notice a training slide with a certain staining flaw is highly influential, and our test slide shares that flaw, the model might be using the flaw as a proxy. Or we might find that all top influences for a non-pCR prediction are slides with very low tumor cellularity – indicating the model has learned “low cellularity => better response” which might correlate with treatment effect (fibrosis replacing tumor) or could be a cohort bias if not careful. • Influence outputs also allow case-based explanation in a different sense: they answer “Have we seen a case like this before and what was its outcome?” If the model points to past patients with similar histology who all failed to respond, that’s valuable context for the clinician (almost like a content-based retrieval of similar patients). Pros: Training-data attribution ties model behavior to real examples in our dataset, which is great for debugging. It ensures transparency about whether the model is relying on legitimate patterns or noise present in the training set . If a model is “memorizing” a cohort-specific artifact, influence functions can reveal that (those artifact-laden slides will show up as influential). Cons: Influence calculations are computationally expensive for large datasets, though we can limit to patch-level features or use approximate methods. They also assume a relatively stable model; if our model was trained with heavy augmentation or non-convex loss, exact influence values have some variance. Nonetheless, even approximate nearest-neighbor analysis in feature space can provide interpretable insights (often the top nearest neighbors in the feature space are essentially what influence functions identify as well). We should treat data attribution findings as indicators rather than absolute truth, and always have a human verify if the “influential” slides share a meaningful pattern with the test slide. Data-Centric Approaches: Morphological Feature Extraction and Batch Correction So far, we discussed methods that probe the model. In parallel, we can use data-centric techniques to translate the raw image into human-interpretable features that help explain the model’s decisions. These are not explanations of the model per se, but they bridge the gap between model features and pathology concepts quantitatively. Nuclei and Tissue Segmentation (HoVer-Net): HoVer-Net is a state-of-the-art tool for simultaneous nuclear segmentation and classification in H&E slides . It can delineate individual nuclei and classify them (e.g. tumor nucleus vs lymphocyte vs stromal). By applying HoVer-Net (or similar) to regions of interest, we can derive metrics: e.g. tumor cell density, lymphocyte density (TIL counts), average nuclear size/shape features, mitotic count per area, etc. These histomorphometric features turn an image region into numbers that pathologists understand. How this aids interpretability: • If the model’s attention map highlights a region, we can compute these features in that region and say, “The model focused on an area with extremely high TIL density and low viable tumor — features consistent with treatment response.” This links the attention heatmap to a semantic description. • We can also correlate slide-level predictions with whole-slide features: e.g., slides predicted as pCR by the model might consistently have higher overall lymphocyte-to-tumor ratios or more necrosis. This kind of correlation can be discovered by regression or cluster analysis on the features (some studies have called these emergent features “digital biomarkers”). • Essentially, segmentation allows quantifying concepts. For example, nuclear segmentation plus classification directly yields tumor-infiltrating lymphocyte density, which we hypothesize is important for pCR. If we confirm that the model’s score correlates with TIL density, it boosts confidence that the model found a real biological signal. Stain Normalization: H&E stain variations between labs and scanners can introduce unwanted biases. A classic approach by Macenko et al. (2009) normalizes slides to a reference stain appearance using optical density and singular value decomposition . By adopting stain normalization in our pipeline, we ensure the model isn’t simply learning color differences. More pertinently for interpretability, it means any heatmap differences are less likely due to one slide being darker or more purple, etc. Normalized slides make the explanations consistent and comparable. If a particular concept (say “eosinophilic stroma”) is truly predictive, it should be so after normalization as well. This is why we will perform analysis with and without normalization to check stability (see Validation section). • We likely should integrate a normalization step (Macenko or alternative like Vahadane) in preprocessing. It’s noted that normalization reduces variability and preserves morphology while standardizing color , which can boost generalization and explanation reliability. Batch Effect Correction (ComBat on Features): Even after color normalization, scanner or site-specific biases can lurk in deep features. ComBat is a technique originally from genomics that can adjust for batch effects by aligning feature distributions across batches. Recent work (Murchan et al., 2024) demonstrated ComBat applied to deep features in pathology can remove scanner/hospital biases without losing predictive signal . This is critical for “right reasons” – we don’t want the model predicting outcome simply because, say, all pCR cases came from Hospital A (and the model picks up Hospital A’s slide processing subtlety). • We will use ComBat to harmonize WSI embeddings across sites. In practice, we can take the embedding vectors (from the foundation model or penultimate layer of our classifier) and apply ComBat to shift them to a common distribution . Then, generate explanations (attention maps, concept attributions) on both original and batch-corrected models. If the explanations change dramatically, that’s a sign some were batch-associated. Ideally, after ComBat, the model’s attention is more uniformly on biology rather than any site-specific patterns • Additionally, during validation we’ll measure performance with and without ComBat to ensure we aren’t hurting accuracy too much while gaining generalization. Quality Control and Artifact Removal (HistoQC): Digital slides often contain artifacts: out-of-focus areas, folds, pen markings, debris, etc. HistoQC is an open-source tool that automatically flags such issues and finds cohort outliers (slides that look fundamentally different) . Incorporating HistoQC in our pipeline ensures that we either remove or down-weight tiles with artifacts before they confuse the model. For interpretation: • By filtering out obvious artifacts, we make the model’s focus more likely to be on real tissue. If an attention map lights up on an edge artifact and we already removed those, that indicates a serious problem (which we can catch in testing). • HistoQC can also be used post-hoc on any highlighted regions: if the model’s top region is one that HistoQC deems blurry or an outlier, we immediately suspect a shortcut. In other words, HistoQC features can serve as “red flags” in our explanation audit. • When reporting explanations, we should note if any identified region had known artifacts. Ideally, after training with QC, none of the important regions are in artifact zones. If they are, we might retrain or explicitly penalize attention on those (ties into the “right reasons training” below). In summary, data-centric techniques ensure the input data is clean and standardized, and they produce auxiliary interpretable features (like cell counts) that help us articulate what the model might be seeing. Combining these with model-focused explanations yields a richer picture: e.g., “Model attention is high in this tumor region which has 90% fibrosis and dense lymphocytes; concurrently, TCAV finds ‘necrosis’ and ‘TIL’ concepts highly indicative of pCR – aligning with the model’s highlighted region.” Robustness and “Right for the Right Reasons” Enhancements Even with the above methods, we must explicitly guard against the model learning spurious correlations or producing misleading explanations. Two techniques are important here: saliency sanity checks and explanation-based model training. Saliency Sanity Checks: A seminal study by Adebayo et al. showed that some saliency methods are alarmingly insensitive to the model and data – for instance, certain gradient-based maps remained almost the same even when model weights were randomized . This means one could be staring at a saliency map thinking it explains the model, when in fact it’s basically an edge detector unrelated to the learned features . We will apply sanity checks to any saliency or attention mechanism we use: • Model randomization test: Shuffle the model weights or labels and verify that the explanation (heatmap or concept importance) changes significantly. If an attention map from a trained model looks similar to one from an untrained model, that map is not trustworthy . We expect truly learned attention to disappear when weights are random. • Data randomization test: Train a model on random labels (outcome labels permuted among slides). This model has no real signal, so any explanation it gives is by definition nonsense. Ensuring our explanation method yields no coherent patterns in this scenario gives confidence it won’t invent structure where there is none • These checks will be part of our validation protocol. If a chosen explanation method fails a sanity check, we would avoid using it for conclusions or apply a different method. Right-for-the-Right-Reasons (RRR) Constraints: This is a family of techniques where we actively guide the model during training to focus on or away from certain things. For example, Ross et al. (2017) introduced a loss term that penalizes the model’s gradient (explanation) on undesired features . In practice, if we know certain regions are spuriously correlated with the label, we can force the model to ignore them by making it costly to base decisions on them. In pathology, this could include: • Background and Slide Borders: If some slides have more background due to how tissue was cut, the model might pick that up. We can add a term: whenever the model gives importance to non-tissue background (which we can segment out easily), add a penalty. This will train the model to keep focus within tissue boundaries. • Scanning Artifacts: If we have annotations or HistoQC outputs for bubbles, pen marks, out-of.focus areas, we can similarly penalize attention or gradient on those. Essentially, we supply a mask $M(x)$ for each image indicating “forbidden regions,” and add a regularization $\lambda | \nabla_{M(x)} f(x)|^2$ to discourage attributions there . • Encourage Known Relevant Regions: Conversely, if domain knowledge or prior studies suggest certain regions should be attended (e.g., tumor-invasive front, lymphoid aggregates), we can mildly regularize the model to pay attention there (this is trickier – we must avoid leaking label information, so it’s more viable in scenarios where we have extra annotations). The effect of RRR training is that the model is less likely to learn shortcuts and more likely to generalize. For example, Ross et al. showed that on a confounded MNIST dataset, adding explanation constraints recovered the true signal and restored accuracy on unbiased test sets . In our case, if we suspect a batch effect or artifact, we can simulate or annotate it and constrain the model accordingly. One could even do this iteratively: train model, use explanations to find a flaw (say, it highlights a slide corner), add constraint and retrain to fix. Pros: RRR directly aligns model training with human priors about relevant vs irrelevant features. It can improve robustness on new data where those spurious cues are absent or altered . It’s a proactive way to get a model that inherently has better explanations (because we nudged it to have the “right” explanations during training). Cons: It requires some prior knowledge of potential shortcuts, and too heavy constraints might harm accuracy. We should use it surgically – for clear-cut confounders. Also, it doesn’t guarantee optimal explanations, just steers away from bad ones. We’ll monitor performance to ensure we’re not over-constraining the model. By combining these methods – attention heatmaps, prototypes, concept analysis, training-data attribution, data-centric feature extraction, normalization/QC, and robust training checks – we cover a broad spectrum of interpretability. The next step is to design experiments to validate that these explanations are doing their job. Validation Experiments and Metrics To ensure our interpretability strategy is effective, we will perform several experiments focused on faithfulness, stability, and clinical plausibility of the explanations, as well as demonstrating the model is “right for the right reasons.” 1. Faithfulness Tests (Causal Impact of Explanations) Ablation Studies: We will systematically ablate (remove) the top-ranked regions according to the model’s explanation and observe the impact on predictions. For example, take the 5% highest-attention tiles in a WSI (or the region with highest prototype activation), mask them out or replace with background, and feed the slide through the model again. We expect the model’s confidence in its original prediction to drop significantly if those regions truly were important. We’ll quantify this as . in predictive score or AUC when top regions are removed versus when random regions are removed. A larger drop for important regions indicates the explanation was faithful to the model’s actual decision basis. Ideally, removing irrelevant regions (low attention) should not change the prediction much, while removing highlighted regions will degrade it appreciably. Concept Perturbation (Counterfactuals): Using concept attribution, we can do virtual experiments: for slides where concept quantifications are available, manipulate a concept and see if the model reacts. For instance, if we have a way to increase “TIL density” in an image (perhaps by copying in more lymphocyte nuclei synthetically, or by using generative models), does the model’s pCR probability rise? Or, easier, use the trained concept bottleneck model: take a test slide’s predicted concept vector and set certain concept values to “absent” or “extreme” and run it through the bottleneck model to see outcome prediction changes . If increasing TIL concept changes prediction to pCR, that supports that concept’s causal role in the model. These counterfactual tests check that our concepts aren’t just correlating with outputs but actually drive the model when adjusted. Synthetic “Right/Wrong” Region Tests: Another faithfulness check: present the model with slides where we manually insert or remove a feature and see if explanations follow. For example, take a slide patch predicted as non-pCR and paste in some necrotic tumor regions from another slide – does the model’s prediction move toward pCR and does the attention shift to the inserted necrosis? Conversely, add an artifact (like a pen mark) to a slide and verify that a good model does not suddenly change its prediction or attention (if it does, it might be overly sensitive to that cue). Metrics: We will report the performance drop (e.g. change in log-odds or C-index) after ablating important regions vs unimportant regions. A faithful explanation method should yield a large delta for important regions . We can also compute the ROAR (RemOve And Retrain) score: remove top X% regions and retrain the model; if accuracy significantly drops compared to removing random regions, it confirms those regions contained critical info. For concept tests, we’ll note qualitative outcomes (e.g. “in 90% of cases, adding tumor-infiltration increased pCR prediction”) as well as any measurable change in probabilities. 2. Stability and Robustness Tests Cross-Domain Consistency: We will assess whether explanations remain stable across varying stain appearances and scanners. Take a set of slides and generate multiple versions: one normalized with Macenko, one with Reinhard, one unnormalized, etc. Run the model and get attention maps or concept attributions for each version. We can then compute similarity metrics between these explanations (e.g. Intersection-over-Union for top-k attention regions, or rank correlation of concept importance scores). If the model is truly focusing on fundamental morphology, the explanations should be consistent regardless of color variations. High dissimilarity would indicate the model might be focusing on color-dependent features (which is undesirable). Our goal is to achieve high explanation agreement across such transformations. Batch Effect Removal Impact: Using the ComBat-corrected model, compare its explanations to the original model’s. For instance, if the original model tended to attend to slide corners for Hospital A slides, after ComBat we’d expect that to disappear. We will measure the distribution of attention pre.and post-ComBat. Another measure: evaluate model performance on an external site before and after feature harmonization 23 . If performance improves and explanations shift to more biologically relevant areas post-harmonization, that’s a win for robustness. We’ll document any reduction in site-specific biases in explanations (perhaps using a site attribution test: train a small classifier to predict site from the model’s attention pattern or features – accuracy should drop after harmonization). Noise and Perturbation Tests: We will add small perturbations to slides – e.g. slight rotations, flips, or adding Gaussian noise – that shouldn’t change the diagnosis, and confirm that the model’s explanation doesn’t fluctuate wildly. This tests explanation stability. We expect that a robust explanation method yields similar outputs (maybe measured by structural similarity index for heatmaps or by concept rank stability) under such perturbations. If we find, say, that moving a slide by 5 pixels causes completely different attention hotspots, that method might be too brittle. Metrics: Key metrics include IoU or correlation of explanation maps under transformations, standard deviation of concept scores across augmentation, and any domain gap in explanations. For example, we can quantify the difference in average attention map between Site A and Site B slides. After applying ComBat/normalization, that difference (perhaps measured in KL divergence of attention distributions) should shrink. Stability metrics will be reported to ensure that our explanations are not an artifact of one specific scanning condition. 3. Clinical Plausibility and Expert Evaluation This aspect evaluates if the explanations make sense to humans (especially pathologists) and adds value. Pathologist Rating of Explanations: We will present a blinded set of cases to board-certified pathologists with the model’s explanations and ask them to rate how plausible they find them. For example, show the WSI with an overlay highlighting the model’s regions of interest (heatmap), plus any concept attributions (“Model indicates high necrosis and low tumor cellularity in this case”). They will score on a Likert scale (1–5) the degree to which the model’s highlighted regions and stated features align with their expectation for what predicts response. We’ll also collect qualitative feedback. If multiple pathologists participate, we can measure inter-rater agreement (e.g. do they consistently find certain explanations useful?). A high average score and good agreement would indicate our explanations are intuitively meaningful. Concordance with Known Biomarkers: We will check if the model’s identified important features correlate with known factors. For instance, it’s known in some cancers that high TILs or complete tumor necrosis post-therapy correlate with pCR. If our model’s explanations frequently point to “many lymphocytes” or “no residual tumor” for pCR cases, that boosts confidence. We might create a confusion matrix of sorts: count how often each concept is marked important by the model in cases that are actually pCR vs non-pCR. If the model is reasonable, “fibrosis/therapy effect” concept should light up in many pCR predictions, whereas “active tumor proliferation” concept might often appear in non-pCR predictions. We can compare such trends with literature and experts’ expectations. Coverage of Explanations: We also evaluate whether the explanations cover the majority of cases. It’s problematic if only, say, 50% of predictions are explainable (e.g., if the model uses some features we understand only in half the cases and something obscure in the rest). So, we’ll measure the fraction of cases where we can assign a clear human-meaningful reason for the model’s call. With multi-method explanations, ideally almost all cases should have at least one clear highlighted morphologic feature or prototype that a pathologist can interpret. If there are cases where even after all methods we’re stumped (“the model focused on something we can’t identify”), those need attention – either they point to a novel pattern (if benign) or a possible flaw (if it’s focusing on noise). Expert–AI Agreement on Regions: For a subset, we can ask pathologists to manually mark regions they think are important for the outcome (independent of the model). Compare that to the model’s attention. Compute overlap – this is like a “precision and recall” of the model’s explanation against expert annotation. While experts might not always agree themselves, a significant overlap would indicate plausibility. If experts highlight completely different regions, the model might be finding something non-obvious (which could be insightful or concerning). Metrics: We’ll summarize average expert rating of explanations, and the proportion of explanations rated as “clinically plausible” (e.g., rating ≥4). We might use Cohen’s kappa to measure agreement among experts on the usefulness of certain explanation types. Additionally, we will report concept-level concordance (e.g., “pathologists expected high TIL in 80% of pCR cases; model’s top concept was TIL in 75% of pCR predictions” – a close match). Another metric could be the percent of cases with at least one high-confidence explanation (where high-confidence might mean the model strongly activates a known concept or prototype and the expert validates it). We aim for this coverage to be as high as possible, indicating the model isn’t relying on enigmas for most predictions. 4. “Right for the Right Reasons” Audits Finally, we’ll explicitly look for evidence of the model using wrong cues and see if our mitigation steps succeeded. Spurious Correlation Checks: Using the influence function results, we will flag any non-biological patterns. For example, if a particular scanner artifact or tissue-fold pattern appears in several top influential patches for model predictions, we note that. We can simulate a “control experiment” by intentionally training a biased model (e.g., one with shuffled labels except those correlated with an artifact) to see what explanations it would give, and ensure our real model’s explanations differ from that. If our QC and RRR measures worked, the model should not systematically attend to, say, slide edges or blank space. We’ll document any instances where it does. Ideally, after interventions, this should be near zero. Right-Reason Training Efficacy: For any constraints we applied during training (like penalizing attention on non-tissue), we will verify their effect. For example, if we penalized attention on slide corners, we compare attention maps before vs after – they should show reduced corner focus. Also, test accuracy on a “confounder-free” subset (if available) should improve with right-reason training . We will report such comparisons. Generalization to External Data: As a final “right reasons” check, evaluate the model on an external test set where some confounders are absent or different (e.g., a new hospital’s data). If our interpretability strategies succeeded, the model should maintain performance and its explanations should remain focused on pathology, not break due to missing shortcuts. This is more of a deployment validation, but it reflects the model’s true reasoning quality. Metrics: Here we will mostly provide qualitative but also some counts: e.g., “0% of high-attention regions overlapped with image corners or pen marks after applying the artifact regularizer, compared to 5% before”. Or “No significant performance drop when slide background was trimmed, indicating model isn’t using slide area as a feature.” We can also track the frequency of “alert” explanations – cases where the model’s focus was on something odd. We expect that frequency to drop after our interventions (and we want it as low as possible). Combining all these validation results, we will have evidence that (a) the explanations truly reflect the model (faithfulness), (b) they hold up across technical variations (stability), and (c) they make sense to humans and correspond to legitimate biology (plausibility). Any gaps identified will inform further model refinement. Recommended Playbook and Implementation Plan Finally, we consolidate the findings into an actionable interpretability playbook for our H&E model, prioritizing methods by impact and ease of implementation: Tier 1 – Immediate Methods to Deploy: 1. Attention Heatmaps with QC: Continue generating MIL attention heatmaps, but integrate HistoQC and stain normalization upfront. This means the input WSIs are color-normalized (e.g., via Macenko) , and tiles with artifacts (as detected by HistoQC) are excluded or marked . The output heatmaps will then highlight artifact-free, normalized tissue regions. We will implement a simple attention threshold to pick out top regions and have the model output those as “important patches” for each slide. These can be reviewed by pathologists in a slide viewer. Success criteria: Heatmap focuses correspond to tumor and microenvironment regions, not obvious noise; removing those patches should affect model output significantly. 2. Concept Activation Analysis (TCAV): Leverage the foundation model’s features to perform TCAV for key concepts . We will start with a small set of pathologist-defined concepts that are hypothesized to influence response: for example, “Dense lymphocytic infiltration,” “Geographic necrosis,” “Poor differentiation,” “High mitotic count,” “Fibrosis (therapy effect).” For each, gather ~20–50 example image patches (we can crowdsource from our slides or public data where those features are present). Run TCAV on our trained model to get concept importance scores. This will immediately produce a human-readable summary like: “Model’s pCR prediction is most sensitive to high necrosis and lymphocytes, and negatively associated with high tumor cellularity,” etc. We will incorporate these findings into reports for each case – e.g., listing which concepts were positively or negatively driving the prediction. Success criteria: TCAV scores align with known pathology (if not, that’s interesting too), and remain stable across different subsets (to ensure they’re not data-order dependent). These concept scores will be part of the model’s output for each slide, giving clinicians a summary of “why” in clinical terms. 3. Prototype Evidence (ProtoPNet-style): As a next step, implement a prototype layer on top of our model’s features. This might require a partial re-training: we freeze the embedding extractor (foundation model) and train a ProtoPNet head that learns, say, 10 prototypes for “responders” and 10 for “non-responders.” Each prototype will correspond to some histologic pattern. We’ll use an existing open-source ProtoPNet implementation for efficiency. After training, for each new slide, we can retrieve the top activated prototypes and show: “Model thinks this region looks like Prototype #7 (which is, e.g., a mucin-rich tumor from a training pCR case).” We can label the prototypes by reviewing their nearest training patches (maybe Prototype 7 = “foamy macrophage area”, Prototype 3 = “high-grade tumor cluster”, etc.). These prototypes become a library of patterns the model considers important. Success criteria: The prototypes are medically meaningful (if many are trivial or uninterpretable, we might prune or reinitialize them), and pathologists find the “this looks like that” explanations helpful in understanding model decisions. We also expect that using prototypes might slightly drop raw accuracy (common with interpretable models), but as long as it remains within a few points and provides insight, it’s acceptable. 4. Batch Effect Mitigation: Include ComBat feature harmonization in the pipeline for training and testing . During training, apply ComBat to the slide embeddings to remove site differences (if sites are known) before the classifier makes its prediction. This forces the model to focus on pathology features common across sites. In deployment, we can apply the same ComBat adjustment to new slide features. Success criteria: Minimal loss in accuracy across-site, improved consistency of explanations (no site-specific oddities), and performance in external validation is maintained or improved. We will document that “ComBat harmonization of deep features effectively reduces the risk of AI models learning confounding site features” as shown in literature . These Tier 1 actions give us a basic but solid explainability framework: clean data in, attention out, plus concept attributions and a first cut of prototypes. Tier 2 – Secondary Enhancements (Next 2–3 months): 1. Influence Function Analysis Tool: Develop an internal tool to compute and visualize influential training examples for a given test slide (using either a subset of data or a fast approximation like nearest neighbors in embedding space, due to full influence being expensive). We don’t need this for every slide in real-time; it’s more for auditing and model improvement cycles. For instance, if a model prediction is contested by a pathologist, we can pull up “the 5 training cases that most strongly pushed the model toward this prediction”. This provides additional context, potentially revealing if the model is leveraging a legitimate analog from the past or something spurious. We will integrate this analysis in our validation report and for any outlier cases during deployment. Success criteria: In at least 90% of cases, a pathologist reviewing the top influences agrees those cases share relevant features with the query (if not, it might indicate the model is grouping things strangely). Also, any systematic spurious influence found can be fed back to retraining (addressed in Tier 3). 2. Extended Prototype/Case Library: Expand the prototype approach to be more flexible. Instead of a fixed ProtoPNet, we could compile a gallery of representative patches for each outcome (or even intermediate concepts). Using clustering on embedded features (unsupervised), we identify common phenotypes in the training data and ensure each is represented in an explanation database. Then for a new slide, we find the nearest patches in this phenotype library and present them. This is like a hybrid of prototype networks and content-based image retrieval. It might help when a slide has a mix of patterns – we could say “This slide has regions similar to prototype A (bad response pattern) and prototype B (good response pattern), the model’s balance between these led to its prediction.” This is advanced and exploratory, but could enhance user trust: the clinician sees not just one prototype but a set of similar past regions with outcomes. 3. Human-in-the-Loop Concept Refinement: Using the results of ACE (concept discovery), we plan a session with pathologists to review unnamed concept clusters. For example, ACE might discover a cluster of patch features that experts identify as “peritumoral lymphoid aggregates.” We then formally name this concept and add it to the TCAV concept list for future analysis. We iteratively build a concept bank. Over time, the model’s behavior can be described in terms of these discovered concepts as well (“Concept #12 (Tumor Budding) was highly activated in this slide, which often indicates aggressive tumor”). Essentially, we grow our interpretability vocabulary in tandem with model learning. Success criteria: We successfully map most of the high-variance components of the model’s feature space to an understandable concept. Fewer “unknown latent factors” remain as the model matures. We’d measure this by how many of the top principal components or cluster centers get a clear annotation. Tier 3 – Longer-Term and Experimental: 1. Concept Bottleneck Model Deployment: Train a concept bottleneck model (or a variant) on our dataset using a limited set of concept labels we can obtain. Even if it’s not as accurate as the main model, deploy it as an auxiliary explainer. For a given slide, the CBM can output predicted values for each concept (like “40% probability of high TILs, 80% probability of necrosis present, etc.”) which is itself a helpful summary. Moreover, we can present counterfactual sliders: “If we imagine this slide had no necrosis, the CBM predicts the probability of pCR would drop from 70% to 30%.” This kind of interactive what-if can be turned into a tool for pathologists to test the model’s reasoning. It also serves as a check: if the CBM’s concept-based prediction agrees with the black-box for the most part, we gain trust that the black-box relies on those concepts. If they diverge, investigate what additional features the black-box might be using. Success criteria: We achieve a CBM accuracy reasonably close to the main model (say within 5-10% AUC), indicating concepts explain a large portion of the variance. Pathologists find the CBM’s output intuitive and possibly even useful as a standalone second opinion (which speaks to the completeness of our concept set). 2. Right-Reason Reinforcement Learning: In cases where we do identify a problematic shortcut (e.g., model notices an inky artifact present in many pCR slides), we will retrain the model with an explanation penalty or mask as discussed. This could be done via a fine-tuning stage: add the constraint, fine-tune the model on training data again. We monitor if the attention on the artifact goes to zero and if performance on validation is stable or improved (particularly on data without the artifact). Over time, this procedure can ferret out biases one by one. We maintain a “bias log” – each time we correct one, we note it, ensuring future models do not regress. Success criteria: After one or two rounds of such fine-tuning, no obvious shortcut uses remain (as per artifact overlap checks and influence inspection). The model’s generalization gap between different cohorts closes, demonstrating it’s focusing on robust features. 3. User Interface & Reporting: Develop a front-end for explanations where for each slide the pathologist or oncologist can see: . The WSI with colored heatmap overlay (toggleable). . A list of top concept attributions for that slide (e.g., “High necrosis (TCAV score 0.9) – contributes to predicted pCR”). . Prototype patch thumbnails that influenced the prediction, with labels or notes (e.g., “Prototype 3: Fibrotic tumor bed from training slide (pCR) – similarity 0.95”). . Optionally, a few similar past cases (in terms of whole-slide features) with outcomes, for context. . The model’s raw prediction and confidence, and possibly confidence intervals if we have an ensemble or Bayesian approximation. This interface would be the tangible product of our interpretability efforts, making the model’s decision process transparent and traceable. We would pilot this interface with friendly users to gather feedback on what explanation format is most useful to them (some might prefer textual explanations, others visual). We’ll iterate to simplify and focus the content (ensuring it doesn’t overwhelm clinicians with too many technical details). Success criteria: Positive user feedback in pilot (e.g., doctors report improved understanding or confidence in the model after using the tool). Also, timed studies could show that explanations help pathologists detect model errors or disagree when the model is likely wrong – meaning the explanations add practical value in review. Risks and Mitigations We should address some potential risks: • Risk: Too much information, user overload. We will prioritize the most informative explanations (likely concept summary and a couple prototypes). User testing will guide what to include. • Risk: Concepts not generalize or not independent. Many histology concepts co-occur (e.g., high TIL often comes with necrosis in treated tumors). TCAV might show both as important where one is the real driver. Mitigation: use multivariate testing (concept importance conditional on another concept present/absent) or concept ablation experiments to tease out causal relations. • Risk: Model changes with updates. When we retrain or get new data, explanations might shift. Our playbook should include re-running TCAV and sanity checks on any new model version, and comparing with prior to ensure consistency or understand differences. This is part of an ongoing monitoring plan. • Risk: Regulatory acceptance. We will document every interpretability technique and its validation, since regulatory bodies (FDA, etc.) will want to see that the model’s decisions can be explained and that mechanisms are in place to detect failures. Our plan includes generating a technical report with sources (like this one) to demonstrate we followed best practices 32 for explainable AI in healthcare. Pilot Results and Next Steps After implementing Tier 1 and some Tier 2 items, we will run a pilot on a retrospective set of cases (with known outcomes) and generate a report of the model’s performance with explanations. This report will include examples such as: • Case 1: Actual pCR, Model predicted pCR. Heatmap shows attention on fibrotic tumor bed and lymphocyte clusters. TCAV scores: High “Treatment Effect (fibrosis)” concept, Low “Residual Tumor” concept. Prototype matched to a training example of scar-like fibrosis with no tumor (pCR case) . Pathologist agrees the slide shows classic therapy response. Interpretation: Model correctly identified therapy-induced fibrosis and absence of tumor – likely basis for its prediction. • Case 2: Actual no-response, Model predicted no-response. Explanation highlights viable tumor regions at the periphery of the resection. Concept analysis: High “Tumor cell density” and “Mitoses” concept importance, Low “Necrosis”. Prototype: a region matched to a training prototype of poorly differentiated carcinoma with mitotic figures (non-responder prototype). This aligns with expectation that residual cellular tumor indicates poor response. • Case 3: Actual pCR, Model predicted pCR but explanations reveal a potential shortcut. Attention map oddly highlights an edge of the tissue section. The top concept was “blue background” (not a real tissue concept). Investigating the training data, we find many pCR slides had a certain trimming artifact that the model latched onto. This case triggers a retraining with an explanation penalty on slide edges. Outcome: After retraining, the model shifts attention to the actual tumor bed (which had some residual tumor it initially ignored – hence the error). This improves the model’s error but importantly removes the unreliable cue. This illustrates how interpretability helped catch and fix a problem. We’ll include several such case studies, alongside aggregate metrics from validation (faithfulness scores, concept correlation with outcome, expert agreement levels, etc.). From the pilot, we expect to demonstrate that: • Our model’s predictions can be explained in terms of known histopathological features in the majority of cases. • The explanations are faithful (removing important features impacts predictions) and robust (not drastically different across stain/batch variations). • We have a process to continually audit and refine the model’s reasoning (using human feedback and technical checks). With these pieces in place, the final deliverable is a robust, interpretable pathology AI system. The model will not only output a risk score or binary prediction, but also a human-readable rationale: e.g., “Predicted pCR with 85% confidence. Supporting findings: extensive tumor necrosis and inflammatory response, minimal residual tumor detected.” Such a system is far more transparent and clinically useful than a black-box probability. It enables pathologists and oncologists to trust and verify the model’s suggestions, thus facilitating adoption in the workflow. By following this interpretability playbook, we ensure our H&E-only predictive model is not a mysterious “oracle” but rather a well-understood assistant that provides evidence for its predictions. This not only helps with user trust and regulatory approval but could also lead to new scientific insights (for example, if the model identifies an image pattern strongly predictive of outcome that was not previously known, that could warrant further investigation). In essence, we move from just predicting outcomes to understanding outcomes through the model’s eyes, which is the ultimate goal of interpretable AI in pathology. Sources: The plan synthesizes best practices and findings from recent literature on WSI interpretability. For attention-based MIL and interpretability, see CLAM by Lu et al. . Concept-based explanations are inspired by TCAV and ACE , and the idea of concept bottlenecks by Koh et al. . Prototype reasoning follows ProtoPNet by Chen et al. (“This looks like that”) . Influence functions come from Koh & Liang . HistoQC for quality control is used as in Janowczyk et al. . Stain normalization is per Macenko’s method , and feature ComBat is per Murchan et al. 2024 (who concluded that ComBat effectively reduces confounding in deep features) . We also incorporate recommendations from Sanity Checks for Saliency Maps (Adebayo et al.) to ensure explanation validity . Finally, right-for.the-right-reasons training is guided by Ross et al. who showed that constraining model explanations can eliminate reliance on confounders. All these pieces combine into our tailored strategy for interpreting H&E-based outcome models in a reliable, pathology-informed manner. (PDF) Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images https://www.researchgate.net/publication/ 340826474_Data_Efficient_and_Weakly_Supervised_Computational_Pathology_on_Whole_Slide_Images Pseudo-class part prototype networks for interpretable breast cancer classification | Scientific Reports https://www.nature.com/articles/s41598-024-60743-x?error=cookies_not_supported&code=1c9aa648.be30-4c58-81f6-006f10d85862 Deep learning generates synthetic cancer histology for explainability and education | npj Precision Oncology https://www.nature.com/articles/s41698-023-00399-4?error=cookies_not_supported&code=ddd32773-25bd-4fab-8a37.a24d73d41d5e PAMIL: Prototype Attention-Based Multiple Instance Learning for ... https://dl.acm.org/doi/10.1007/978-3-031-72083-3_34 Transparency of deep neural networks for medical image analysis https://www.sciencedirect.com/science/article/pii/S0010482521009057 Discovering Hierarchical Explanations for Pathology Foundation ... https://dl.acm.org/doi/abs/10.1007/978-3-032-04978-0_21 Concept Bottleneck Models https://proceedings.mlr.press/v119/koh20a.html CampusAI https://campusai.github.io/papers/Understanding-Black-box-Predictions-via-Infuence-Functions [1812.06499] HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images https://arxiv.org/abs/1812.06499 Understanding Stain Normalization in Histopathology: Reinhard, Macenko, and Vahadane Methods | by Olanrewaju Charles | Medium https://medium.com/@olanrewaju.charles2014/understanding-stain-normalization-in-histopathology-reinhard-macenko.and-vahadane-methods-1cce4a5a6f8e Deep feature batch correction using ComBat for machine learning applications in computational pathology - PubMed https://pubmed.ncbi.nlm.nih.gov/39398947/ HistoQC: An Open-Source Quality Control Tool for Digital Pathology ... https://ascopubs.org/doi/10.1200/CCI.18.00157 Sanity Checks for Saliency Maps http://papers.neurips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations https://www.ijcai.org/proceedings/2017/0371.pdf 